
network
	inherit torch.nn.Module
		e.g.
			class Linear(nn.Module):
    				def __init__(self, in_dim, out_dim):
        					super(Linear, self).__init__()
					# Parameter: A kind of Tensor that is to be considered a module parameter, requiring grad as default
        					self.w = nn.parameter.Parameter(torch.randn(in_dim, out_dim))
        					self.b = nn.parameter.Parameter(torch.randn(out_dim))
    
    				def forward(self, x):
        					y = x @ self.w
        					z = y + b
        					return z

			
			class Perceptron(nn.Module):
   				def __init__(self, in_dim, hidden_dim, out_dim):
        					super(Linear, self).__init__()
					# sub Module
        					self.layer1 = Linear(in_dim, hidden_dim)
        					self.layer2 = Linear(hidden_dim, out_dim)
    				def forward(self,x):
        					y = self.layer1(x)
        					z = torch.sigmoid(x)
        					return self.layer2(z)			


			L = Linear(4, 2)
			x = torch.randn(3, 4)
			o = L(x)

	torch.nn.Sequential
		e.g.
			sq1 = nn.Sequential()
			sq1.add_module('conv1', nn.Conv2d(1, 1, 3))
			sq1.add_module('conv2', nn.Conv2d(1, 1, 3))

			sq2 = nn.Sequential(
    			nn.Conv2d(1, 1, 3),
    			nn.Conv2d(1, 1, 3)
			)

			from collections import OrderedDict
			sq3 = nn.Sequential(OrderedDict({'conv2d': nn.Conv2d(1, 1, 3)}))


	torch.nn.ModuleList 
		like a regular Python list, but modules it contains are properly registered

		e.g.
			class MyModule(nn.Module):
    				def __init__(self):
        					super(MyModule, self).__init__()
       					self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])

    				def forward(self, x):
        					for i, l in enumerate(self.linears):
            					x = self.linears[i // 2](x) + l(x)
       			 	return x

	from torch import optim
		e.g.
			# set lr for subnetwork
			optimizer =optim.SGD([
                				{'params': net.features.parameters()},		# lr = 1e-3
                				{'params': net.classifier.parameters(), 'lr': 1e-1}
            				], lr=1e-3)


			# modify lr without constructing a new optimizer
			for param_group in optimizer.param_groups:
    				param_group['lr'] *= 0.1

	nn.Dropout and nn.functional.dropout
		nn.Dropout doesn't drop in eval, while latter does

	init
		e.g.
			from torch.nn import init

			init.xavier_normal_(net.weight)

	save and load
		e.g.
			torch.save(net.state_dict(), 'net.pth')
			n = net()
			n.load_state_dict(torch.load('net.pth')

			# another way
			torch.save(net, 'n.pth')
			n = torch.load('n.pth')

	run on GPU
			m = model.cuda()
			x = input.cuda()


			# parallel
			new_net = nn.DataParallel(net, device_ids=[0, 1])
			output = new_net(input)
			# or
			output = nn.parallel.data_parallel(net, input, device_ids=[0, 1])
			